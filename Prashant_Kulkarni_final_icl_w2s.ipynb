{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/prashantkul/weak-to-strong-gen/blob/main/Prashant_Kulkarni_final_icl_w2s.ipynb)\n",
    "\n",
    "# Weak-to-Strong Generalization Experiments\n",
    "\n",
    "**Click the badge above to run this notebook in Google Colab!**",
    "\n\n---\n\n",
    "\n",
    "This notebook demonstrates how to clone the repository from git and run all experiments.\n",
    "\n",
    "**What this notebook does:**\n",
    "- Clones the code repository from GitHub\n",
    "- Installs all dependencies\n",
    "- Sets up environment and configuration\n",
    "- Runs all 6 experiments (baseline, disclaimer, CoT \u00d7 2 models)\n",
    "- Generates comprehensive visualization\n",
    "\n",
    "**Estimated runtime:** 75-90 minutes total\n",
    "\n",
    "**Requirements:**\n",
    "- OpenRouter API key (get from https://openrouter.ai/settings/keys)\n",
    "- Python 3.10+\n",
    "- ~$150-200 in API credits (if running all experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Repository Setup\n",
    "\n",
    "This section clones the repository and installs dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Clone Repository from GitHub\n",
    "# This will download all the code from git\n",
    "# Replace prashantkul/weak-to-strong-gen with your actual GitHub repository\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if we're in Colab\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    # Clone repository to Colab\n",
    "    !git clone https://github.com/prashantkul/weak-to-strong-gen.git\n",
    "    %cd astra\n",
    "    REPO_PATH = Path('/content/astra')\n",
    "else:\n",
    "    print(\"Running in local Jupyter\")\n",
    "    # For local Jupyter, assume we're already in the repo directory\n",
    "    # If not, uncomment the lines below and update the path\n",
    "    # !git clone https://github.com/prashantkul/weak-to-strong-gen.git\n",
    "    # %cd astra\n",
    "    REPO_PATH = Path.cwd()\n",
    "\n",
    "print(f\"Repository path: {REPO_PATH}\")\n",
    "print(f\"Repository exists: {REPO_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Install Dependencies\n",
    "# This installs all required Python packages\n",
    "\n",
    "print(\"Installing dependencies...\")\n",
    "print(\"This may take 2-3 minutes\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Install from requirements.txt\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\u2713 Dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Add Repository to Python Path\n",
    "# This allows us to import the src modules\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add repository to path if not already there\n",
    "repo_path = str(REPO_PATH)\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.insert(0, repo_path)\n",
    "    print(f\"\u2713 Added {repo_path} to Python path\")\n",
    "else:\n",
    "    print(f\"\u2713 {repo_path} already in Python path\")\n",
    "\n",
    "# Verify we can import the modules\n",
    "try:\n",
    "    from src import Config, DatasetManager, ExperimentRunner\n",
    "    from notebook_experiments import run_baseline_sweep, run_disclaimer_sweep, run_cot_sweep\n",
    "    print(\"\u2713 Successfully imported all modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u2717 Import failed: {e}\")\n",
    "    print(\"Make sure you're in the repository directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Environment Configuration\n",
    "\n",
    "This section sets up API keys and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Set API Keys\n",
    "# For Google Colab: Use Colab Secrets (RECOMMENDED)\n",
    "\n",
    "import os\n",
    "\n",
    "# Check if running in Colab\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ\n",
    "\n",
    "if IN_COLAB:\n",
    "    # COLAB METHOD: Use Secrets\n",
    "    # 1. Click the key icon (\ud83d\udd11) in the left sidebar\n",
    "    # 2. Add two secrets:\n",
    "    #    - Name: OPENROUTER_API_KEY, Value: your primary key\n",
    "    #    - Name: OPENROUTER_API_KEY_BACKUP, Value: your backup key\n",
    "    # 3. Enable notebook access for both secrets\n",
    "    # 4. Run this cell\n",
    "    \n",
    "    from google.colab import userdata\n",
    "    os.environ['OPENROUTER_API_KEY'] = userdata.get('OPENROUTER_API_KEY')\n",
    "    os.environ['OPENROUTER_API_KEY_BACKUP'] = userdata.get('OPENROUTER_API_KEY_BACKUP')\n",
    "    print(\"\u2713 Loaded API keys from Colab Secrets\")\n",
    "    \n",
    "else:\n",
    "    # LOCAL JUPYTER: Use .env file\n",
    "    from dotenv import load_dotenv\n",
    "    if load_dotenv():\n",
    "        print(\"\u2713 Loaded API keys from .env file\")\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f No .env file found\")\n",
    "        print(\"\\nTo use locally:\")\n",
    "        print(\"  1. Copy .env.example to .env\")\n",
    "        print(\"  2. Edit .env and add your API keys\")\n",
    "        print(\"  3. Restart kernel and re-run this cell\")\n",
    "\n",
    "# Verify keys are set\n",
    "if os.getenv('OPENROUTER_API_KEY'):\n",
    "    key_preview = os.getenv('OPENROUTER_API_KEY')[:15] + \"...\"\n",
    "    print(f\"\u2713 Primary API key set: {key_preview}\")\n",
    "else:\n",
    "    print(\"\u2717 OPENROUTER_API_KEY not set!\")\n",
    "    print(\"\\n\u26a0\ufe0f Please set up your API keys before continuing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Initialize Configuration\n",
    "# Load configuration and set up the environment\n",
    "\n",
    "from src import Config\n",
    "\n",
    "# Load configuration from environment variables\n",
    "config = Config.from_env()\n",
    "config.setup_environment()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Weak Model:     {config.weak_model}\")\n",
    "print(f\"Strong Model:   {config.strong_model}\")\n",
    "print(f\"Temperature:    {config.temperature}\")\n",
    "print(f\"Max Parallel:   {config.max_parallel_requests}\")\n",
    "print(f\"Cache Dir:      {config.cache_dir}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\u2713 Configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Dataset Loading\n",
    "\n",
    "Load and verify the TruthfulQA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Load Dataset\n",
    "# This loads the TruthfulQA dataset and splits it into test and few-shot pool\n",
    "\n",
    "from src import DatasetManager\n",
    "\n",
    "print(\"Loading TruthfulQA dataset...\")\n",
    "print(\"This may take 30-60 seconds on first load\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "dm = DatasetManager()\n",
    "test_data, few_shot_pool, split = dm.load_split()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATASET LOADED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Set Size:      {len(test_data)} questions\")\n",
    "print(f\"Few-Shot Pool Size: {len(few_shot_pool)} questions\")\n",
    "print(f\"Split Used:         {split}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show example question\n",
    "print(\"\\nExample Question:\")\n",
    "print(test_data[0].question)\n",
    "print(f\"\\nCorrect Answer: {test_data[0].answer}\")\n",
    "print(\"\\n\u2713 Dataset loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Quick Test (Optional)\n",
    "# Run a quick test to verify everything is working\n",
    "# This will make one API call to test the setup\n",
    "\n",
    "from test_notebook_functions import test_notebook_setup\n",
    "\n",
    "print(\"Running quick test...\")\n",
    "print(\"This will make one API call to verify the setup\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_passed = await test_notebook_setup()\n",
    "\n",
    "if test_passed:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\u2713 ALL TESTS PASSED\")\n",
    "    print(\"\u2713 Ready to run experiments!\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\u2717 TESTS FAILED\")\n",
    "    print(\"Please check the errors above and fix them before proceeding\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Baseline Experiments\n",
    "\n",
    "Run baseline experiments for both model pairs (8B\u2192405B and 8B\u219270B) across K\u2208{0,2,5,10}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Run 405B Baseline Experiments\n",
    "# Tests how 405B performs with weak supervision at different K values\n",
    "# Expected runtime: 15-20 minutes\n",
    "\n",
    "from notebook_experiments import run_baseline_sweep\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 1: 405B BASELINE\")\n",
    "print(\"=\"*70)\n",
    "print(\"Testing 8B\u2192405B with K={0,2,5,10}\")\n",
    "print(\"Expected runtime: 15-20 minutes\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_405b_baseline = await run_baseline_sweep(\n",
    "    model_pair=\"8b_to_405b\",\n",
    "    k_values=[0, 2, 5, 10],\n",
    "    save_results=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2713 405B BASELINE COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nResults Summary:\")\n",
    "for k, result in results_405b_baseline['pgr_results'].items():\n",
    "    print(f\"  K={k}: PGR = {result.pgr:.3f} ({result.pgr_percentage})\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Run 70B Baseline Experiments\n",
    "# Tests how 70B performs with weak supervision at different K values\n",
    "# Expected runtime: 10-15 minutes\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 2: 70B BASELINE\")\n",
    "print(\"=\"*70)\n",
    "print(\"Testing 8B\u219270B with K={0,2,5,10}\")\n",
    "print(\"Expected runtime: 10-15 minutes\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_70b_baseline = await run_baseline_sweep(\n",
    "    model_pair=\"8b_to_70b\",\n",
    "    k_values=[0, 2, 5, 10],\n",
    "    save_results=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2713 70B BASELINE COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nResults Summary:\")\n",
    "for k, result in results_70b_baseline['pgr_results'].items():\n",
    "    print(f\"  K={k}: PGR = {result.pgr:.3f} ({result.pgr_percentage})\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compare the two models\n",
    "print(\"\\nComparison at K=10:\")\n",
    "pgr_405b = results_405b_baseline['pgr_results'][10].pgr\n",
    "pgr_70b = results_70b_baseline['pgr_results'][10].pgr\n",
    "print(f\"  405B: {pgr_405b:.3f} ({pgr_405b*100:.1f}%)\")\n",
    "print(f\"  70B:  {pgr_70b:.3f} ({pgr_70b*100:.1f}%)\")\n",
    "print(f\"  Gap:  {(pgr_405b - pgr_70b)*100:.1f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Disclaimer Experiments\n",
    "\n",
    "Run disclaimer experiments that add a metacognitive warning about weak label quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Run 405B Disclaimer Experiments\n",
    "# Tests if warning about weak labels helps 405B\n",
    "# Expected runtime: 10 minutes\n",
    "\n",
    "from notebook_experiments import run_disclaimer_sweep\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 3: 405B DISCLAIMER\")\n",
    "print(\"=\"*70)\n",
    "print(\"Testing metacognitive prompt for 405B\")\n",
    "print(\"Expected runtime: 10 minutes\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find the baseline experiment path\n",
    "baseline_405b_path = results_405b_baseline['experiment_path']\n",
    "print(f\"Using baseline from: {baseline_405b_path}\\n\")\n",
    "\n",
    "results_405b_disclaimer = await run_disclaimer_sweep(\n",
    "    model_pair=\"8b_to_405b\",\n",
    "    k_values=[0, 2, 5, 10],\n",
    "    baseline_exp_path=baseline_405b_path,\n",
    "    save_results=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2713 405B DISCLAIMER COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nResults Summary (Delta from Baseline):\")\n",
    "for k, result in results_405b_disclaimer['pgr_results'].items():\n",
    "    baseline_pgr = results_405b_baseline['pgr_results'][k].pgr\n",
    "    delta = result.pgr - baseline_pgr\n",
    "    sign = \"+\" if delta >= 0 else \"\"\n",
    "    print(f\"  K={k}: PGR = {result.pgr:.3f} (\u0394 = {sign}{delta:.3f})\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Run 70B Disclaimer Experiments\n",
    "# Tests if warning about weak labels helps 70B\n",
    "# Expected runtime: 8 minutes\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 4: 70B DISCLAIMER\")\n",
    "print(\"=\"*70)\n",
    "print(\"Testing metacognitive prompt for 70B\")\n",
    "print(\"Expected runtime: 8 minutes\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find the baseline experiment path\n",
    "baseline_70b_path = results_70b_baseline['experiment_path']\n",
    "print(f\"Using baseline from: {baseline_70b_path}\\n\")\n",
    "\n",
    "results_70b_disclaimer = await run_disclaimer_sweep(\n",
    "    model_pair=\"8b_to_70b\",\n",
    "    k_values=[0, 2, 5, 10],\n",
    "    baseline_exp_path=baseline_70b_path,\n",
    "    save_results=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2713 70B DISCLAIMER COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nResults Summary (Delta from Baseline):\")\n",
    "for k, result in results_70b_disclaimer['pgr_results'].items():\n",
    "    baseline_pgr = results_70b_baseline['pgr_results'][k].pgr\n",
    "    delta = result.pgr - baseline_pgr\n",
    "    sign = \"+\" if delta >= 0 else \"\"\n",
    "    print(f\"  K={k}: PGR = {result.pgr:.3f} (\u0394 = {sign}{delta:.3f})\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show K-dependent reversal\n",
    "print(\"\\n\u26a0\ufe0f Notice the K-dependent reversal:\")\n",
    "for k in [0, 2, 5, 10]:\n",
    "    baseline_pgr = results_70b_baseline['pgr_results'][k].pgr\n",
    "    disc_pgr = results_70b_disclaimer['pgr_results'][k].pgr\n",
    "    delta = disc_pgr - baseline_pgr\n",
    "    effect = \"helps\" if delta > 0 else \"hurts\" if delta < 0 else \"neutral\"\n",
    "    print(f\"  K={k}: {effect} ({delta:+.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Chain-of-Thought Label Generation\n",
    "\n",
    "Generate reasoning demonstrations for CoT experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Generate Weak CoT Labels (8B with Reasoning)\n",
    "# Generate reasoning demonstrations from the weak 8B model\n",
    "# Expected runtime: 5 minutes\n",
    "\n",
    "from src import ModelEvaluator, get_model_pair\n",
    "from src.model_evaluator import ModelResponse\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GENERATING WEAK COT LABELS (8B)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Generating reasoning demonstrations from 8B model\")\n",
    "print(\"Expected runtime: 5 minutes\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get weak model\n",
    "pair = get_model_pair(\"8b_to_405b\")\n",
    "weak_model_id = pair.weak_model\n",
    "\n",
    "# Create evaluator with CoT enabled\n",
    "evaluator = ModelEvaluator(config, use_cot=True)\n",
    "\n",
    "# Generate labels for first 20 examples from few-shot pool\n",
    "questions_to_label = few_shot_pool[:20]\n",
    "questions = [(q.question_id, q.question) for q in questions_to_label]\n",
    "\n",
    "print(f\"\\nGenerating reasoning for {len(questions)} questions...\\n\")\n",
    "\n",
    "weak_cot_responses = await evaluator.evaluate_batch(\n",
    "    questions=questions,\n",
    "    model_id=weak_model_id,\n",
    "    few_shot_prompt=None,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Calculate accuracy\n",
    "gt_map = {q.question_id: q.answer for q in questions_to_label}\n",
    "num_correct = sum(1 for r in weak_cot_responses if r.answer == gt_map[r.question_id])\n",
    "accuracy = num_correct / len(weak_cot_responses)\n",
    "\n",
    "print(f\"\\n8B CoT Accuracy: {accuracy:.1%} ({num_correct}/{len(weak_cot_responses)})\")\n",
    "\n",
    "# Save to data directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = Path(\"data/cot_weak_labels\")\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "output_file = output_dir / f\"8b_cot_weak_labels_{timestamp}.json\"\n",
    "\n",
    "data = {\n",
    "    \"metadata\": {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"weak_model\": weak_model_id,\n",
    "        \"temperature\": config.temperature,\n",
    "        \"num_labels\": len(weak_cot_responses),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"use_cot\": True\n",
    "    },\n",
    "    \"weak_labels\": [r.model_dump() for r in weak_cot_responses]\n",
    "}\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "weak_cot_path = str(output_file)\n",
    "print(f\"\\n\u2713 Saved to: {weak_cot_path}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Generate Gold CoT Labels (405B with Reasoning)\n",
    "# Generate reasoning demonstrations from the strong 405B model\n",
    "# Expected runtime: 10 minutes\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GENERATING GOLD COT LABELS (405B)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Generating reasoning demonstrations from 405B model\")\n",
    "print(\"Expected runtime: 10 minutes\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get strong model\n",
    "strong_model_id = pair.strong_model\n",
    "\n",
    "print(f\"\\nGenerating reasoning for {len(questions)} questions...\\n\")\n",
    "\n",
    "gold_cot_responses = await evaluator.evaluate_batch(\n",
    "    questions=questions,\n",
    "    model_id=strong_model_id,\n",
    "    few_shot_prompt=None,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Calculate accuracy\n",
    "num_correct = sum(1 for r in gold_cot_responses if r.answer == gt_map[r.question_id])\n",
    "accuracy = num_correct / len(gold_cot_responses)\n",
    "\n",
    "print(f\"\\n405B CoT Accuracy: {accuracy:.1%} ({num_correct}/{len(gold_cot_responses)})\")\n",
    "\n",
    "# Save to data directory\n",
    "output_dir = Path(\"data/cot_gold_labels\")\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "output_file = output_dir / f\"405b_cot_gold_labels_{timestamp}.json\"\n",
    "\n",
    "data = {\n",
    "    \"metadata\": {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"strong_model\": strong_model_id,\n",
    "        \"temperature\": config.temperature,\n",
    "        \"num_labels\": len(gold_cot_responses),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"use_cot\": True\n",
    "    },\n",
    "    \"gold_labels\": [r.model_dump() for r in gold_cot_responses]\n",
    "}\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "gold_cot_path = str(output_file)\n",
    "print(f\"\\n\u2713 Saved to: {gold_cot_path}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Chain-of-Thought Experiments\n",
    "\n",
    "Run CoT experiments using the reasoning demonstrations generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Run 405B CoT Experiments\n",
    "# Tests if reasoning demonstrations help 405B\n",
    "# Expected runtime: 10 minutes\n",
    "\n",
    "from notebook_experiments import run_cot_sweep\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 5: 405B CHAIN-OF-THOUGHT\")\n",
    "print(\"=\"*70)\n",
    "print(\"Testing CoT with reasoning demonstrations for 405B\")\n",
    "print(\"Expected runtime: 10 minutes\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_405b_cot = await run_cot_sweep(\n",
    "    model_pair=\"8b_to_405b\",\n",
    "    k_values=[0, 2, 5, 10],\n",
    "    baseline_exp_path=baseline_405b_path,\n",
    "    weak_cot_labels_path=weak_cot_path,\n",
    "    gold_cot_labels_path=gold_cot_path,\n",
    "    save_results=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2713 405B COT COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nResults Summary (Delta from Baseline):\")\n",
    "for k, result in results_405b_cot['pgr_results'].items():\n",
    "    baseline_pgr = results_405b_baseline['pgr_results'][k].pgr\n",
    "    delta = result.pgr - baseline_pgr\n",
    "    sign = \"+\" if delta >= 0 else \"\"\n",
    "    print(f\"  K={k}: PGR = {result.pgr:.3f} (\u0394 = {sign}{delta:.3f})\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show crossover effect\n",
    "print(\"\\n\u26a0\ufe0f Notice the crossover effect:\")\n",
    "for k in [0, 2, 5, 10]:\n",
    "    baseline_pgr = results_405b_baseline['pgr_results'][k].pgr\n",
    "    cot_pgr = results_405b_cot['pgr_results'][k].pgr\n",
    "    delta = cot_pgr - baseline_pgr\n",
    "    effect = \"helps\" if delta > 0.02 else \"hurts\" if delta < -0.02 else \"neutral\"\n",
    "    print(f\"  K={k}: {effect} ({delta:+.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Run 70B CoT Experiments\n",
    "# Tests if reasoning demonstrations help 70B\n",
    "# Expected runtime: 8 minutes\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 6: 70B CHAIN-OF-THOUGHT\")\n",
    "print(\"=\"*70)\n",
    "print(\"Testing CoT with reasoning demonstrations for 70B\")\n",
    "print(\"Expected runtime: 8 minutes\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_70b_cot = await run_cot_sweep(\n",
    "    model_pair=\"8b_to_70b\",\n",
    "    k_values=[0, 2, 5, 10],\n",
    "    baseline_exp_path=baseline_70b_path,\n",
    "    weak_cot_labels_path=weak_cot_path,\n",
    "    gold_cot_labels_path=gold_cot_path,\n",
    "    save_results=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2713 70B COT COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nResults Summary (Delta from Baseline):\")\n",
    "for k, result in results_70b_cot['pgr_results'].items():\n",
    "    baseline_pgr = results_70b_baseline['pgr_results'][k].pgr\n",
    "    delta = result.pgr - baseline_pgr\n",
    "    sign = \"+\" if delta >= 0 else \"\"\n",
    "    print(f\"  K={k}: PGR = {result.pgr:.3f} (\u0394 = {sign}{delta:.3f})\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show that CoT consistently hurts 70B\n",
    "print(\"\\n\u26a0\ufe0f Notice CoT consistently hurts 70B at K>0:\")\n",
    "for k in [0, 2, 5, 10]:\n",
    "    baseline_pgr = results_70b_baseline['pgr_results'][k].pgr\n",
    "    cot_pgr = results_70b_cot['pgr_results'][k].pgr\n",
    "    delta = cot_pgr - baseline_pgr\n",
    "    effect = \"helps\" if delta > 0 else \"hurts\" if delta < 0 else \"neutral\"\n",
    "    print(f\"  K={k}: {effect} ({delta:+.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Comprehensive Visualization\n",
    "\n",
    "Create final 6-panel comparison visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Generate Comprehensive Comparison Visualization\n",
    "# Creates a 6-panel visualization comparing all experiments\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CREATING COMPREHENSIVE VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run the visualization script\n",
    "!python create_final_comparison.py\n",
    "\n",
    "print(\"\\n\u2713 Visualization created: results/final_comprehensive_comparison.png\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Display Visualization\n",
    "# Show the comprehensive comparison\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"Displaying comprehensive comparison...\\n\")\n",
    "display(Image('results/final_comprehensive_comparison.png', width=1200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 9: Results Summary\n",
    "\n",
    "Display final results table with all experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Create Results Summary Table\n",
    "# Display all results in a comprehensive table\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPLETE RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create summary data\n",
    "summary_data = []\n",
    "for k in [0, 2, 5, 10]:\n",
    "    summary_data.append({\n",
    "        \"K\": k,\n",
    "        \"405B Baseline\": f\"{results_405b_baseline['pgr_results'][k].pgr:.3f}\",\n",
    "        \"405B Disclaimer\": f\"{results_405b_disclaimer['pgr_results'][k].pgr:.3f}\",\n",
    "        \"405B CoT\": f\"{results_405b_cot['pgr_results'][k].pgr:.3f}\",\n",
    "        \"70B Baseline\": f\"{results_70b_baseline['pgr_results'][k].pgr:.3f}\",\n",
    "        \"70B Disclaimer\": f\"{results_70b_disclaimer['pgr_results'][k].pgr:.3f}\",\n",
    "        \"70B CoT\": f\"{results_70b_cot['pgr_results'][k].pgr:.3f}\",\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display deltas from baseline\n",
    "print(\"\\nDELTA FROM BASELINE (Intervention Effect)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "delta_data = []\n",
    "for k in [0, 2, 5, 10]:\n",
    "    delta_data.append({\n",
    "        \"K\": k,\n",
    "        \"405B Disclaimer\": f\"{results_405b_disclaimer['pgr_results'][k].pgr - results_405b_baseline['pgr_results'][k].pgr:+.3f}\",\n",
    "        \"405B CoT\": f\"{results_405b_cot['pgr_results'][k].pgr - results_405b_baseline['pgr_results'][k].pgr:+.3f}\",\n",
    "        \"70B Disclaimer\": f\"{results_70b_disclaimer['pgr_results'][k].pgr - results_70b_baseline['pgr_results'][k].pgr:+.3f}\",\n",
    "        \"70B CoT\": f\"{results_70b_cot['pgr_results'][k].pgr - results_70b_baseline['pgr_results'][k].pgr:+.3f}\",\n",
    "    })\n",
    "\n",
    "delta_df = pd.DataFrame(delta_data)\n",
    "print(delta_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 10: Key Findings\n",
    "\n",
    "Summary of novel research findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Display Key Findings\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "\ud83d\udd2c FINDING 1: Scaling Threshold for Robustness\n",
    "   - 405B maintains PGR \u2265 0.984 across all K values\n",
    "   - 70B degrades to PGR = 0.864 at K=10\n",
    "   - Threshold exists between 70B and 405B parameters (~6\u00d7 difference)\n",
    "\n",
    "\ud83d\udd2c FINDING 2: K-Dependent Reversal Effect (Disclaimer)\n",
    "   - Disclaimer helps at low K (+0.080 at K=2 for 70B)\n",
    "   - Disclaimer hurts at high K (-0.017 at K=10 for 70B)\n",
    "   - First documented reversal in metacognitive prompting!\n",
    "\n",
    "\ud83d\udd2c FINDING 3: CoT Crossover Effect\n",
    "   - 405B: CoT hurts at K=0 (-0.164), helps at K>0 (+0.095 at K=2)\n",
    "   - 70B: CoT consistently hurts at K>0\n",
    "   - Large models can filter noisy reasoning, medium models cannot\n",
    "\n",
    "\ud83d\udd2c FINDING 4: Superelicitation\n",
    "   - 405B with CoT at K=10: PGR = 1.048 (>100%)\n",
    "   - Weak supervision + reasoning can exceed gold supervision quality\n",
    "\n",
    "\"\"\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\u2705 All experiments complete!\")\n",
    "print(\"\\nFor detailed findings, see: KEY_FINDINGS.md\")\n",
    "print(\"For visualization: results/final_comprehensive_comparison.png\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Updating Code from Git\n",
    "\n",
    "To pull the latest changes from the repository in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Pull Latest Changes from Git (Optional)\n",
    "# Run this cell to update the code to the latest version\n",
    "\n",
    "print(\"Pulling latest changes from git...\")\n",
    "!git pull origin main\n",
    "\n",
    "print(\"\\n\u26a0\ufe0f After pulling updates, restart the kernel to reload modules:\")\n",
    "print(\"   Jupyter: Kernel > Restart\")\n",
    "print(\"   Colab: Runtime > Restart runtime\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}